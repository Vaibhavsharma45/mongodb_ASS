{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f81b6144",
   "metadata": {},
   "source": [
    "# Theoretical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e168972",
   "metadata": {},
   "source": [
    "# 1. What are the key differences between SQL and NoSQL databases?\n",
    "The primary differences lie in their data model, schema, scalability, and query language.\n",
    "Data Model: SQL databases are relational (like MySQL, PostgreSQL) and store data in tables with rows and columns. NoSQL databases are non-relational and can have various data models, such as document (MongoDB), key-value (Redis), column-family (Cassandra), or graph (Neo4j).\n",
    "Schema: SQL databases have a predefined, rigid schema. You must define the table structure before inserting data. NoSQL databases typically have a dynamic schema, allowing you to insert documents without a predefined structure, which offers great flexibility.\n",
    "Scalability: SQL databases are designed to scale vertically, meaning you increase performance by adding more resources (CPU, RAM) to a single server. NoSQL databases are built to scale horizontally, meaning you add more servers to distribute the load, which is often more cost-effective for large-scale applications.\n",
    "Query Language: SQL databases use the powerful and standard Structured Query Language (SQL). NoSQL databases have their own unique query languages or APIs, which can vary significantly between different databases.\n",
    "\n",
    "# 2. What makes MongoDB a good choice for modern applications?\n",
    "MongoDB is well-suited for modern applications due to its:\n",
    "Flexible Document Model: It stores data in JSON-like BSON documents, which can be easily mapped to objects in modern programming languages. This flexibility allows for rapid development and iteration as application requirements change. \n",
    "High Scalability: MongoDB is designed for horizontal scaling through a process called sharding, allowing it to handle massive amounts of data and high traffic loads.\n",
    "High Availability: Through replica sets, MongoDB provides automatic failover and data redundancy, ensuring that applications remain online even if a server fails.\n",
    "Rich Query Language: It supports a powerful query language, indexing, and real-time aggregation capabilities that allow for sophisticated data analysis.\n",
    "\n",
    "# 3. Explain the concept of collections in MongoDB.\n",
    "A collection in MongoDB is a grouping of BSON documents. It is the equivalent of a table in a relational (SQL) database.\n",
    "Collections exist within a database.\n",
    "Unlike SQL tables, collections do not enforce a schema by default. This means documents within the same collection can have different fields and structures.\n",
    "For example, you could have an users collection where some documents have a middle_name field and others do not.\n",
    "\n",
    "# 4. How does MongoDB ensure high availability using replication?\n",
    "MongoDB ensures high availability through replica sets. A replica set is a group of connected MongoDB instances (mongod processes) that maintain the same dataset.\n",
    "A replica set consists of:\n",
    "One Primary Node: This is the main node that receives all write operations.\n",
    "Multiple Secondary Nodes: These nodes replicate the data from the primary node. They can be used for read operations to distribute the load.\n",
    "If the primary node becomes unavailable, the remaining secondary nodes will automatically elect a new primary from among themselves. This failover process ensures that the database remains operational with minimal downtime. üõ°Ô∏è\n",
    "\n",
    "# 5. What are the main benefits of MongoDB Atlas?\n",
    "MongoDB Atlas is the official cloud database-as-a-service (DBaaS) for MongoDB. Its main benefits are:\n",
    "Fully Managed: Atlas automates time-consuming administrative tasks like database setup, configuration, patching, and backups.\n",
    "Multi-Cloud Deployment: It allows you to deploy your database on major cloud providers like AWS, Google Cloud, and Azure, preventing vendor lock-in.\n",
    "Global Distribution: You can easily deploy clusters across different geographic regions to reduce latency for users worldwide.\n",
    "Built-in Security: It provides robust security features out-of-the-box, including network isolation, encryption at rest and in transit, and role-based access control.\n",
    "Scalability: You can easily scale your database up or down with just a few clicks or via an API call, without any downtime.\n",
    "\n",
    "# 6. What is the role of indexes in MongoDB, and how do they improve performance?\n",
    "Indexes in MongoDB store a small portion of the collection's data in an easy-to-traverse form. The role of an index is to make queries faster. ‚ö°\n",
    "Without an index, MongoDB must perform a collection scan, meaning it has to look through every single document in a collection to find the ones that match the query. As the collection grows, this becomes very slow.\n",
    "When you create an index on a specific field, MongoDB creates a sorted data structure. When you query that field, MongoDB can use the index to quickly locate the relevant documents without scanning the entire collection, dramatically improving query performance.\n",
    "\n",
    "# 7. Describe the stages of the MongoDB aggregation pipeline.\n",
    "The aggregation pipeline is a framework for data analysis in MongoDB. It consists of a sequence of stages, where each stage transforms the documents as they pass through it. The output of one stage becomes the input for the next.\n",
    "Common stages include:\n",
    "$match: Filters documents, similar to a find() query, allowing only matching documents to pass to the next stage.\n",
    "$group: Groups documents by a specified identifier and applies accumulator expressions (e.g., $sum, $avg, $max ) to the grouped data.\n",
    "$project: Reshapes documents by adding new fields, removing existing fields, or renaming fields.\n",
    "$sort: Sorts the documents based on a specified field.\n",
    "$limit: Restricts the number of documents passed to the next stage.\n",
    "$lookup: Performs a left outer join to another collection in the same database.\n",
    "\n",
    "# 8. What is sharding in MongoDB?\n",
    "How does it differ from replication?\n",
    "Sharding is MongoDB's method for horizontal scaling. It involves distributing data across multiple servers or clusters, known as shards. Each shard holds a subset of the total data, allowing the database to handle larger datasets and higher throughput than a single server could manage. Sharding is used to solve problems of data size and traffic volume.\n",
    "The key difference between sharding and replication is their purpose:\n",
    "Replication is for high availability and redundancy. It copies the entire dataset to multiple servers. Its goal is to prevent data loss and ensure the system remains online if a server fails.\n",
    "Sharding is for scalability. It splits the dataset into parts and distributes those parts across multiple servers. Its goal is to handle large amounts of data and high loads by distributing the work.\n",
    "A production environment typically uses both sharding and replication: each shard is itself a replica set to ensure both scalability and high availability.\n",
    "\n",
    "# 9. What is PyMongo, and why is it used?\n",
    "PyMongo is the official and most widely used Python driver for MongoDB. It's a library that allows Python applications to connect to and interact with a MongoDB database. \n",
    "It is used to perform all database operations from a Python script, including:\n",
    "Connecting to a MongoDB instance or cluster.\n",
    "Creating, reading, updating, and deleting (CRUD) documents.\n",
    "Executing aggregation pipelines.\n",
    "Managing indexes and collections.\n",
    "\n",
    "# 10. What are the ACID properties in the context of MongoDB transactions?\n",
    "Starting from version 4.0, MongoDB supports multi-document ACID transactions on replica sets. The ACID properties ensure data integrity:\n",
    "Atomicity: Transactions are \"all or nothing.\" If any operation within the transaction fails, the entire transaction is aborted, and the database is left unchanged.\n",
    "Consistency: A transaction brings the database from one valid state to another. Any data written to the database must be valid according to all defined rules, including schema validation.\n",
    "Isolation: Transactions that run concurrently produce the same result as if they were executed serially (one after another). MongoDB achieves this using snapshot isolation.\n",
    "Durability: Once a transaction is successfully committed, the changes are permanent and will survive any subsequent system failures, such as a power outage or crash.\n",
    "\n",
    "# 11. What is the purpose of MongoDB's explain() function?\n",
    "The explain() function is a diagnostic tool used to get detailed information about how MongoDB executes a query. When you append .explain() to a query (like find() or aggregate()), it doesn't return the query results but instead provides a report. \n",
    "This report includes details like:\n",
    "Which index, if any, was used for the query.\n",
    "Whether a collection scan was performed.\n",
    "The number of documents scanned versus the number of documents returned.\n",
    "The execution time and other performance metrics.\n",
    "Developers use explain() to diagnose and optimize slow queries, for instance, by seeing if an index is being used effectively.\n",
    "\n",
    "# 12. How does MongoDB handle schema validation?\n",
    "While MongoDB is known for its flexible schema, you can enforce a specific structure using schema validation. This feature allows you to define rules that documents must follow to be inserted or updated in a collection.\n",
    "Schema validation is defined on a per-collection basis using the $jsonSchema operator. You can specify:\n",
    "The data type of fields (e.g., bsonType: \"string\").\n",
    "Required fields in a document.\n",
    "Ranges for numerical values.\n",
    "Regular expressions for string patterns.\n",
    "You can also set the validation level to strict (apply rules to all inserts/updates) or moderate (apply rules only to valid documents), and choose whether to error or just warn when a document fails validation.\n",
    "\n",
    "# 13. What is the difference between a primary and a secondary node in a replica set?\n",
    "In a MongoDB replica set:\n",
    "Primary Node: This is the master node. There can only be one primary node at any given time. It is the only node that can accept write operations (inserts, updates, deletes). All changes are recorded in its operation log (oplog).\n",
    "Secondary Node(s): These are the slave nodes. They asynchronously replicate data from the primary's oplog to maintain an identical copy of the dataset. Secondaries cannot accept write operations but can be used to serve read traffic, helping to distribute the load. They also participate in elections to choose a new primary if the current one fails.\n",
    "\n",
    "# 14. What security mechanisms does MongoDB provide for data protection?\n",
    "MongoDB provides a comprehensive set of security features to protect data:\n",
    "Authentication: Verifies the identity of users connecting to the database. MongoDB supports mechanisms like SCRAM (default), x.509 certificates, and LDAP.\n",
    "Authorization (Role-Based Access Control): Restricts what authenticated users are allowed to do. You can assign built-in or custom roles to users that grant specific privileges (e.g., read-only, read-write) on certain databases or collections.\n",
    "Encryption:\n",
    "In-Transit Encryption: Uses TLS/SSL to encrypt all data moving between the client and the server and between nodes in a cluster.\n",
    "At-Rest Encryption: Encrypts the database files on disk, available in the Enterprise version or through cloud providers like Atlas.\n",
    "Client-Side Field Level Encryption: Allows applications to encrypt specific fields in a document before sending them to the database, so the server never sees the sensitive data in plaintext.\n",
    "Auditing: Records administrative and data access events on the database for security analysis.\n",
    "\n",
    "# 15. Explain the concept of embedded documents and when they should be used.\n",
    "An embedded document (or sub-document) is a document that is nested inside another document within a MongoDB collection. This creates a \"one-to-many\" or \"one-to-few\" relationship where the \"many\" or \"few\" items are stored within the \"one\" parent document.\n",
    "Example: Instead of separate user and address collections, you can embed address information directly within the user document:\n",
    "JSON\n",
    "{\n",
    "  \"_id\": 1,\n",
    "  \"name\": \"John Doe\",\n",
    "  \"email\": \"john@example.com\",\n",
    "  \"addresses\": [\n",
    "    { \"type\": \"home\", \"street\": \"123 Main St\", \"city\": \"Anytown\" },\n",
    "    { \"type\": \"work\", \"street\": \"456 Business Rd\", \"city\": \"Businesstown\" }\n",
    "  ]\n",
    "}\n",
    "\n",
    "When to use them:\n",
    "For \"contains\" or \"has-a\" relationships where the embedded data doesn't have a reason to exist on its own.\n",
    "When you need to retrieve the related data in a single database query. Embedding avoids the need for joins and can improve read performance significantly.\n",
    "When the \"many\" side is not excessively large and doesn't grow unboundedly. If an array of embedded documents grows too large, it can hit the 16MB BSON document size limit and hurt performance.\n",
    "\n",
    "# 16. What is the purpose of MongoDB's $lookup stage in aggregation?\n",
    "The $lookup stage is used in an aggregation pipeline to perform a left outer join with another collection in the same database.\n",
    "Its purpose is to de-normalize data by pulling in documents from another collection and adding them to the input documents. For example, if you have an orders collection and a products collection, you could use $lookup to join orders with products based on a product_id field to get the full product details for each order. üîó\n",
    "\n",
    "# 17. What are some common use cases for MongoDB?\n",
    "MongoDB's flexibility and scalability make it suitable for a wide range of use cases, including:\n",
    "Content Management Systems (CMS): Easily store articles, blog posts, and user comments with varying fields.\n",
    "E-commerce Applications: Manage product catalogs where different products have different attributes, and handle user profiles and shopping carts.\n",
    "Real-time Analytics: Use the aggregation framework to process and analyze large volumes of data for dashboards and reports.\n",
    "Internet of Things (IoT): Ingest and process high-velocity time-series data from sensors and devices.\n",
    "Single View Applications: Aggregate data from multiple systems into a single, comprehensive view of a customer or entity.\n",
    "\n",
    "# 18. What are the advantages of using MongoDB for horizontal scaling?\n",
    "The main advantages of using MongoDB's horizontal scaling (sharding) are:\n",
    "Increased Storage Capacity: By distributing data across multiple machines, you can store much larger datasets than would be possible on a single server.\n",
    "Increased Throughput: Both read and write operations can be distributed across the shards, allowing the database to handle a higher load of concurrent requests.\n",
    "High Availability: When combined with replica sets (each shard is a replica set), you get both the scalability of sharding and the redundancy of replication.\n",
    "Cost-Effectiveness: Scaling out by adding more commodity servers is often cheaper than scaling up by purchasing a single, more powerful and expensive server.\n",
    "\n",
    "# 19. How do MongoDB transactions differ from SQL transactions?\n",
    "While both provide ACID guarantees, there are key differences:\n",
    "Scope: In MongoDB, the need for multi-document transactions is less frequent due to the document model, which often allows related data to be updated atomically within a single document. In SQL, related data is typically spread across multiple tables, making transactions essential for most operations.\n",
    "Syntax and Implementation: The syntax is different. MongoDB transactions are initiated on a session object (e.g., with session.start_transaction():) and have specific API calls. SQL transactions use commands like BEGIN TRANSACTION, COMMIT, and ROLLBACK.\n",
    "Performance: Because of the distributed nature of MongoDB, multi-document transactions can introduce higher latency compared to single-document operations or typical SQL transactions on a single node. It's recommended to use them only when necessary.\n",
    "\n",
    "# 20. What are the main differences between capped collections and regular collections?\n",
    "Size: A capped collection has a fixed maximum size in bytes. A regular collection has no size limit other than available disk space.\n",
    "Behavior: Once a capped collection reaches its maximum size, it starts behaving like a circular buffer. The oldest documents are automatically overwritten to make space for new ones (First-In, First-Out). Regular collections simply grow as new data is added.\n",
    "Insertion Order: Capped collections maintain the insertion order of documents. Queries on a capped collection return documents in this order by default.\n",
    "Use Case: Capped collections are ideal for high-throughput scenarios where you only need to store recent data, such as logging, caching, or real-time event streaming. Regular collections are used for general-purpose data storage.\n",
    "\n",
    "# 21. What is the purpose of the $match stage in MongoDB's aggregation pipeline?\n",
    "The $match stage is used to filter documents in an aggregation pipeline. It works just like a find() query. Only the documents that satisfy the specified conditions are passed to the next stage in the pipeline.\n",
    "Placing $match at the beginning of a pipeline is a crucial performance optimization, as it reduces the amount of data that subsequent stages need to process.\n",
    "\n",
    "# 22. How can you secure access to a MongoDB database?\n",
    "Securing access involves a layered approach:\n",
    "Enable Authentication: Turn on access control so that only authenticated users can connect.\n",
    "Enforce Authorization: Use Role-Based Access Control (RBAC) to give users only the permissions they need (Principle of Least Privilege).\n",
    "Encrypt Communication: Use TLS/SSL to encrypt all network traffic between clients and the database.\n",
    "Limit Network Exposure: Configure firewalls to allow connections only from trusted IP addresses. Avoid exposing your database directly to the public internet.\n",
    "Keep MongoDB Updated: Regularly apply security patches and updates to protect against known vulnerabilities.\n",
    "\n",
    "# 23. What is MongoDB's WiredTiger storage engine, and why is it important?\n",
    "WiredTiger is the default storage engine for MongoDB. A storage engine is the component of the database responsible for managing how data is stored, retrieved, and managed on disk and in memory.\n",
    "WiredTiger is important because it provides several key features that are critical for modern, high-performance applications:\n",
    "Document-Level Concurrency: It uses optimistic concurrency control, allowing multiple clients to read and write to different documents in a collection simultaneously without conflict, which significantly improves performance for concurrent workloads.\n",
    "Compression: It supports compression libraries like Snappy (default) and zlib, which reduce the amount of disk space needed to store data and can improve I/O performance.\n",
    "In-Memory Caching: It has a highly efficient caching mechanism that keeps the most frequently accessed data in RAM for faster retrieval.\n",
    "Snapshots and Checkpoints: It supports stable checkpoints, making it durable and allowing for recovery from crashes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b5ae95",
   "metadata": {},
   "source": [
    "# MONGO DB PRACTICAL QUESTION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f4424c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Write a Python script to load the Superstore dataset from a CSV file into MongoDB.\n",
    "#ans:-\n",
    "import pymongo\n",
    "import csv\n",
    "\n",
    "# 1. Connect to MongoDB\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"superstore_db\"]\n",
    "collection = db[\"orders\"]\n",
    "\n",
    "# 2. Clear old data\n",
    "collection.delete_many({})\n",
    "\n",
    "# 3. Read CSV and insert into MongoDB\n",
    "csv_file_path = 'superstore.csv'\n",
    "data_to_insert = []\n",
    "\n",
    "# Open file with the correct encoding\n",
    "with open(csv_file_path, mode='r', encoding='latin-1') as file:\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    for row in csv_reader:\n",
    "        # Convert numeric fields\n",
    "        try:\n",
    "            row['Sales'] = float(row.get('Sales', 0))\n",
    "            row['Quantity'] = int(row.get('Quantity', 0))\n",
    "            row['Discount'] = float(row.get('Discount', 0))\n",
    "            row['Profit'] = float(row.get('Profit', 0))\n",
    "        except (ValueError, TypeError):\n",
    "            pass \n",
    "        data_to_insert.append(row)\n",
    "\n",
    "if data_to_insert:\n",
    "    collection.insert_many(data_to_insert)\n",
    "    print(f\"Successfully loaded {len(data_to_insert)} documents.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13be4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. Retrieve and print all documents from the Orders collection.\n",
    "#ans:-\n",
    "# find({}) with an empty dictionary retrieves all documents\n",
    "all_orders = collection.find({})\n",
    "\n",
    "# The result is a cursor, which we can iterate over\n",
    "for order in all_orders:\n",
    "    print(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb982777",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Count and display the total number of documents in the Orders collection\n",
    "#ans:-\n",
    "# count_documents({}) with an empty filter counts all documents\n",
    "total_documents = collection.count_documents({})\n",
    "\n",
    "print(f\"Total number of documents in the Orders collection: {total_documents}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774c35ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Write a query to fetch all orders from the \"West\" region. \n",
    "#ans:-\n",
    "# The filter {\"Region\": \"West\"} specifies the condition\n",
    "west_region_orders = collection.find({\"Region\": \"West\"})\n",
    "\n",
    "print(\"Orders from the West region:\")\n",
    "for order in west_region_orders:\n",
    "    print(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88b6eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Write a query to find orders where Sales is greater than 500.\n",
    "#ans:-\n",
    "# The $gt operator stands for \"greater than\"\n",
    "high_sales_orders = collection.find({\"Sales\": {\"$gt\": 500}})\n",
    "\n",
    "print(\"Orders with Sales greater than 500:\")\n",
    "for order in high_sales_orders:\n",
    "    print(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b669ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Fetch the top 3 orders with the highest Profit. \n",
    "#ans:-\n",
    "# We use sort() to order the results and limit() to get the top N\n",
    "# pymongo.DESCENDING or -1 is used for descending order\n",
    "top_3_profit_orders = collection.find({}).sort(\"Profit\", pymongo.DESCENDING).limit(3)\n",
    "\n",
    "print(\"Top 3 orders with the highest profit:\")\n",
    "for order in top_3_profit_orders:\n",
    "    print(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197b0cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. Update all orders with Ship Mode as \"First Class\" to \"Premium Class.\"\n",
    "#ans:-\n",
    "# The first dictionary is the filter, the second is the update operation\n",
    "# $set is used to update the value of a field\n",
    "update_result = collection.update_many(\n",
    "    {\"Ship Mode\": \"First Class\"},\n",
    "    {\"$set\": {\"Ship Mode\": \"Premium Class\"}}\n",
    ")\n",
    "\n",
    "print(f\"Matched {update_result.matched_count} documents and modified {update_result.modified_count} documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21be35e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. Delete all orders where Sales is less than 50\n",
    "#ans:-\n",
    "# The $lt operator stands for \"less than\"\n",
    "delete_result = collection.delete_many({\"Sales\": {\"$lt\": 50}})\n",
    "\n",
    "print(f\"Deleted {delete_result.deleted_count} documents where Sales was less than 50.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcf8c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#9. Use aggregation to group orders by Region and calculate total sales per region.\n",
    "#ans:-\n",
    "# The aggregation pipeline is a list of stages\n",
    "pipeline = [\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$Region\",  # Group by the \"Region\" field\n",
    "            \"TotalSales\": {\"$sum\": \"$Sales\"}  # Calculate the sum of \"Sales\" for each group\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$sort\": {\"TotalSales\": -1} # Optional: sort by total sales\n",
    "    }\n",
    "]\n",
    "\n",
    "sales_by_region = collection.aggregate(pipeline)\n",
    "\n",
    "print(\"Total sales per region:\")\n",
    "for region_data in sales_by_region:\n",
    "    print(region_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c2527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#10. Fetch all distinct values for Ship Mode from the collection\n",
    "#ans:-\n",
    "# The distinct() method returns a list of unique values for a given field\n",
    "distinct_ship_modes = collection.distinct(\"Ship Mode\")\n",
    "\n",
    "print(\"Distinct Ship Modes:\")\n",
    "print(distinct_ship_modes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586c807e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#11. Count the number of orders for each category.\n",
    "#ans:-\n",
    "# This aggregation pipeline groups by \"Category\" and counts the documents in each group\n",
    "pipeline = [\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$Category\",\n",
    "            \"OrderCount\": {\"$sum\": 1} # For each document in the group, add 1 to the count\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "orders_per_category = collection.aggregate(pipeline)\n",
    "\n",
    "print(\"Number of orders per category:\")\n",
    "for category_count in orders_per_category:\n",
    "    print(category_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92603c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Close the connection\n",
    "client.close()\n",
    "print(\"Connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8cda55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range: 40\n",
      "Variance: 200.0\n",
      "Standard deviation: 14.142135623730951\n"
     ]
    }
   ],
   "source": [
    "#variance, standard deviation, range\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = [10,20,30,40,50]\n",
    "\n",
    "# Range\n",
    "data_range = np.max(data) - np.min(data)\n",
    "\n",
    "variance = np.var(data,ddof = 0)\n",
    "standard_deviation = np.std(data,ddof = 0)\n",
    "\n",
    "\n",
    "print(\"Range:\",data_range)\n",
    "print(\"Variance:\",variance)\n",
    "print(\"Standard deviation:\",standard_deviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d437ff",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa99676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Cluster:--  Cluster1\n",
      "Sample:--  [1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "#cluster sampling \n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "population = {\n",
    "    \"Cluster1\": [1, 2, 3, 4, 5],\n",
    "    \"Cluster2\": [6, 7, 8, 9, 10],\n",
    "    \"Cluster3\": [11, 12, 13, 14, 15]\n",
    "}\n",
    "selected_cluster = random.choice(list(population.keys()))\n",
    "sample = population[selected_cluster]\n",
    "print(\"Selected Cluster:-- \", selected_cluster)\n",
    "print(\"Sample:-- \", sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d5010e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
